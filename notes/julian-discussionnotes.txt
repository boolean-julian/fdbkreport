With the given setup of our case study, the two-finger rotation gesture appeared more often than the pinch-zoom gesture (see Figure \ref{fig:tfr_v_pz}). We assume users attempt to use the pinch-zoom gesture when an object on the screen is too small to see or use. Given this, there was no need for our participants to step closer to the objects, because they started right in front of the objects. Also, for simplicity's sake, the prototypes were not created with unnecessarily small controls, which means all of them were well visible in general. It is possible that our results for two-finger rotations are applicable in a similar way for pinch-zoom.
	
	FIRST PARAGRAPH
	maybe add "This would however require further investigation."

We have to divide the evaluation of the rotation into two groups, as we do for our \emph{supportive} feedback implementation (see \nameref{sec:approach}). While most of the users intuitively walked around objects to rotate their view on the scene, they struggled more with rotating a certain object, namely a piece of bread in this case. This is the reason why the second task for the toaster prototype is most important. The specific task is just ``Toast the bread'', but this includes rotating the object laying on the plate (see \ref{fig:feedbackonphone} A) by $90\degree$ to put it into the device. The struggle with this task also becomes visible by the fact that only 10\% of the participants were able to do it intuitively without trying a two-finger rotation before. Therefore most users could profit from helpful feedback.

	SECOND PARAGRAPH

	Past tense? (we already divided into groups etc.)
	
	1st sentence: Why do we *have* to divide it like that? Just say we did it that way and not that we had to maybe
	
	then maybe continue like so: ... struggled when trying to rotate the pieces of bread. There were two occurences of tasks in which an object had to be rotated in order to complete them, namely the second and third toaster tasks (ref to task table) [don't mention task name again]. However, our results suggest that once the participants learned how to rotate the toasts in the second toaster task, most of them knew how to accomplish said rotation in the third toaster task [insert numbers of failed tries for the two maybe]. Therefore we focus on the second toaster task. 

	another thing: The specific task is just ``Toast the bread'', but this includes rotating the object laying on the plate (see \ref{fig:feedbackonphone} A) by $90\degree$ to put it into the device. <-- this and other such observations concerning why we chose the tasks we did choose most likely belong into the case study setup, because we do not need know the results in order to tell what subtasks our tasks include. it was just basic assumption when we designed the study. putting it here would either mean mentioning it twice or mentioning it in the wrong place imho.

	last sentences:
		* "the struggle" is probably actually the "inability to rotate objects"
		* "also becomes visible by the fact that" sounds weird to my german almost functional brain.
			* 	about "also": where *else* does it become visible tho?
		* "to do it" -> "to complete the task"
		* "to"
		* "could profit" -> maybe "might profit", because it is less ambiguous
							("could" ist auch irgendwie vergangenheit und nicht nur konditional glaub ich)
		* "helpful" -> "appropriate", helpful kinda almost suggests the message content in a way (don't it be kinda like that?)

The feedback versions we provided performed differently in the case study. While the implementation with \emph{critique} feedback and the implementation with \emph{combined} feedback did not had a significant effect, the users with the \emph{support} feedback were able to fulfill the task in less time and with significantly less attempts of the incorrect two-finger rotation gestures. Prior knowledge about the usage of \ac{AR} applications can not be a reason for it, because this groups participants had the lowest percentage of prior knowledge. Instead we can assume multiple reasons for this difference. One is that the \emph{support} feedback implementation has a better visibility than the other two (see \ref{fig:feedbackonphone}). While the other two implementations provide messages looking like the typical Android pop-up messages, the \emph{support} feedback messages are larger and are colored orange instead of gray. Additional the messages of this implementation are shown for a longer duration. All this changes are there to make the messages more eye-catching for the users. This matches with our observation during the test that some of the participants with the \emph{critique} or \emph{combined} ignored the messages, which did not happen for participants with the \emph{support} feedback. The other difference is the helpfulness of the messages' content. The \emph{critique} feedback just tells the users what they have done incorrect, but not how they do it correctly. Since the actual rotation gesture was not intuitive for most participants, they could not come up with the right idea for rotation easily themselves. Our \emph{combined} feedback implementation has a similar issue. The message popping for any kind of rotation is \emph{``Rotating the object is not possible. Try moving around the object.''}. This message does not provide the right help for the most challenging task, which is the rotation of the piece of bread, because it is designed for the wrong kind of rotation. The \emph{support} feedback message serves better with \emph{``Hold the object and move the phone to rotate''}, because its specific enough to help in the given situation.			

	THIRD PARAGRAPH

	* "did not HAVE a significant effect"
	
	instead maybe:
	according to our results, prior knowledge about AR applications cannot account for this effect [these effects?], because the support feedback groups had the lowest percentage of participants with prior knowledge in them.

	* can assume -> assume
	* multiple reasons or fancily "a multitude of reasons" hEHEHE
	* put "colored orange instead of gray" instead
	
	* additionally
	* all this changes -> all these changes
	* of this implementation -> in this implementation maybe?
	* eye-catching -> noticeable
	* matches with our -> matches our
	* orange messages were always noticed? bold statement, is this true? holy shmoly
	
	* "what they have done incorrectly, but not how to do it correctly instead"
	* sentence with "right idea" sounds little nonchalant

	* do not quote the message, refer to the table instead after saying what is important about the message
	* does not provide fitting/appropriate/matching support? ("matching" the situation)
	* "designed for the wrong kind of rotation" what does that mean
	* maybe this instead?

		When a two-finger rotation is detected, a message is provided. In this implemenentation, the content of the message does not depend on whether the rotation was executed on an object that can be moved or not (ref to msg table). This indifference causes the message to not provide appropriate support for each situation. This fact concerns the most difficult task, which according to the number of failed gesture attempts and completion times is the second toaster task.
		Since the support implementation clearly discriminates between two-finger rotations executed on movable and unmovable objects (ref to msg table again), the feedback messages are specific enough to help in any given situation.


Our data states that participants perform significantly better with the \emph{support} feedback implementation, but we also asked the participants to assess their experience with our \ac{AR} application themselves. The results of our \ac{SUS} questionnaire cannot clearly show a difference between the case study groups for the different implementations (see Figure \ref{fig:sus}). So the impact on how the participants feel the usability is not as strong as the impact on how they actually perform. We asked the participants to assess the usability of the overall AR application, since that is what the feedback messages should serve to. Unfortunately other aspects have an impact on the general experience too. Also the users strongly rated the usability of the prototype itself, because we tested the usability of our feedback with the use of the prototypes. That is why our case study groups are too small to evaluate the \ac{SUS} results. The answers to the open question fit to this impression. We asked them to give us opinions directly on the feedback the application provided or did not provide, in case of the control group. Still almost as many participants reacted to the app usage (\emph{app controls}) as to the \emph{app feedback} as shown in Figure \ref{fig:tags}.


FOURTH PARAGRAPH
	* "feel the usability" - "see the usability"/"rate the usability"
	* "since the feedback messages were targeted at increasing the overall usability of [mobile AR?] apps"
	* "other aspects" - describe pls (at least examples i.e. quality of graphics (lighting and shading), controls, ...)
	
	* "that is why" that does not seem like a valid reason. maybe better:
		With the number of participants we tested however, no significant effect of our feedback on the overall usability according to the SUS can be reported. Given a larger sample size it might be possible to find such effects.

	* "strongly rated" -> "often rated" or describe the influence this (partial) rating had on the total rating
		* 	already mention what you mention later here (The answers to the open question fit to this impression)
			maybe like so:
				The results from the open question, which was directly pointed at the feedback we provide (or do not provide in the case of the control group), suggest that the participants might have often rated the quality of the prototype itself rather than the overall application in the course of the SUS questionnaire (ref to fig), just like they did in the open question.
